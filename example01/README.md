 #  Docker container and REST APIs for online inference`

**Source**: [Machine Learning Prediction in Real Time Using Docker and Python REST APIs with Flask](https://towardsdatascience.com/machine-learning-prediction-in-real-time-using-docker-and-python-rest-apis-with-flask-4235aa2395eb)

> **Online Inference** is the process of generating machine learning predictions in real time upon request. It is also known as real time inference or dynamic inference. Typically, these predictions are generated on a single observation of data at runtime. Predictions generated using online inference may be generated at any time of the day.

> **Batch inference**, or offline inference, is the process of generating predictions on a batch of observations. The batch jobs are typically generated on some recurring schedule (e.g. hourly, daily). These predictions are then stored in a database and can be made available to developers or end users. Batch inference may sometimes take advantage of big data technologies such as Spark to generate predictions. This allows data scientists and machine learning engineers to take advantage of scalable compute resources to generate many predictions at once.

**Credit:** [ML in Production - Batch Inference vs Online Inference](https://mlinproduction.com/batch-inference-vs-online-inference/)